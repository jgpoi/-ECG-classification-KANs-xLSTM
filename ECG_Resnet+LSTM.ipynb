{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12388094,"sourceType":"datasetVersion","datasetId":7811510}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"7a3d4edc-4ad6-40ef-9c06-99c320a50b46","cell_type":"code","source":"!pip install ptflops\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import accuracy_score,f1_score, confusion_matrix, recall_score, precision_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom ptflops import get_model_complexity_info\nimport time\n# 1. æ•°æ®åŠ è½½ä¸Žé¢„å¤„ç†\n# è¯»å–æ•°æ®\ndata = pd.read_csv('/kaggle/input/mitbih/ECG_data.csv')\n\n# æå–ä¿¡å·å’Œæ ‡ç­¾\nsignals = data['Signal'].apply(lambda x: np.fromstring(x[1:-1], sep=',')).values\nlabels = data['Label'].values\n\n# è½¬æ¢ä¿¡å·ä¸ºNumPyæ•°ç»„\nsignals = np.array([np.array(signal) for signal in signals])\n\n# æ ‡ç­¾ç¼–ç \nlabel_encoder = LabelEncoder()\nlabels_encoded = label_encoder.fit_transform(labels)\n# æ•°æ®åˆ†å‰²\nX_train, X_temp, y_train, y_temp = train_test_split(\n    signals, labels_encoded, test_size=0.3, random_state=42, stratify=labels_encoded)\n\n# ç¬¬äºŒæ­¥ï¼šå°†ä¸´æ—¶é›†ä¸­çš„ 2/3 è®¾ä¸ºéªŒè¯é›†ï¼ˆ20%ï¼‰ï¼Œ1/3 è®¾ä¸ºæµ‹è¯•é›†ï¼ˆ10%ï¼‰\nX_val, X_test, y_val, y_test = train_test_split(\n    X_temp, y_temp, test_size=1/3, random_state=42, stratify=y_temp)\n\n# æ£€æŸ¥åˆ’åˆ†æ¯”ä¾‹\nprint(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n\n# 2. è½¬æ¢è¾“å…¥æ ¼å¼\nX_train = X_train.reshape(-1, 1, 300)\n# æ¯ä¸ªæ ·æœ¬300ä¸ªæ—¶é—´æ­¥ï¼Œæ¯ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªç‰¹å¾ (channels=1, seq_len=300)\nX_val = X_val.reshape(-1, 1, 300)\nX_test = X_test.reshape(-1, 1, 300)\n\n\n# è½¬æ¢ä¸ºTensor\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\nX_test_tensor = torch.tensor(X_test, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train, dtype=torch.long)\ny_val_tensor = torch.tensor(y_val, dtype=torch.long)\ny_test_tensor = torch.tensor(y_test, dtype=torch.long)\n\n# åˆ›å»ºDataLoader\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\nval_dataset = TensorDataset(X_val_tensor, y_val_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\n# 3. æž„å»º ResNet + LSTM æ¨¡åž‹ï¼ˆPyTorchå®žçŽ°ï¼‰\nclass ResNetLSTM(nn.Module):\n    def __init__(self, input_size=1, hidden_size=64, num_classes=5):\n        super(ResNetLSTM, self).__init__()\n        self.conv1 = nn.Conv1d(input_size, 32, kernel_size=3, padding=1)\n        self.dropout = nn.Dropout(0.2)\n        \n        # ResNet block\n        self.resnet_block1 = self._resnet_block(32, 32)\n        self.resnet_block2 = self._resnet_block(32, 32)\n        self.resnet_block3 = self._resnet_block(32, 32)\n        self.resnet_block4 = self._resnet_block(32, 32)\n        \n        # Global feature learning layer (Global Average Pooling)\n        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # Global feature vector\n        \n        # LSTM layer\n        self.lstm = nn.LSTM(32, hidden_size, batch_first=True)\n        \n        # Fully connected layer for classification\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def _resnet_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.dropout(x)\n        \n        # Passing through ResNet blocks\n        residual = x\n        x = self.resnet_block1(x)\n        x = x + residual\n        residual = x\n        x = self.resnet_block2(x)\n        x = x + residual\n        residual = x\n        x = self.resnet_block3(x)\n        x = x + residual\n        residual = x\n        x = self.resnet_block4(x)\n        x = x + residual\n        \n        # Global feature learning\n        x = self.global_avg_pool(x)\n        \n        # LSTM\n        x, (h_n, c_n) = self.lstm(x.view(x.size(0), -1, 32))  # Flatten the tensor before LSTM\n        x = x[:, -1, :]  # Only take the output of the last time step\n        \n        # Fully connected layer for classification\n        x = self.fc(x)\n        return x\n\n# Instantiate the model\nmodel = ResNetLSTM(input_size=1, hidden_size=64, num_classes=5)\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2, num_classes=5):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.num_classes = num_classes\n        self.epsilon = 1e-6  # é˜²æ­¢é™¤é›¶é”™è¯¯\n\n    def forward(self, inputs, targets):\n        inputs = torch.softmax(inputs, dim=1)\n        targets = torch.zeros_like(inputs).scatter_(1, targets.view(-1, 1), 1)\n        p_t = (inputs * targets).sum(dim=1) + self.epsilon\n        loss = -self.alpha * (1 - p_t) ** self.gamma * torch.log(p_t)\n        return loss.mean()\n\n# 4. è®¾ç½®æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ çŽ‡è°ƒåº¦å™¨\ncriterion = FocalLoss(alpha=0.25, gamma=2, num_classes=5)  # ä½¿ç”¨ Focal Loss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # æ¯10è½®å­¦ä¹ çŽ‡å‡åŠ\n# é€‰æ‹©è®¾å¤‡\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef compute_test_metrics(model, data_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    all_targets = []\n    all_predictions = []\n    start_time = time.time()  # âœ… å¼€å§‹è®¡æ—¶\n    \n    with torch.no_grad():\n        for data, target in data_loader:\n            data = data.to(device)\n            target = target.to(device)\n            output = model(data)\n            _, predicted = torch.max(output, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n            all_targets.extend(target.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n            \n    end_time = time.time()  # âœ… ç»“æŸè®¡æ—¶\n    elapsed_time = end_time - start_time\n    accuracy = 100 * correct / total\n    f1 = f1_score(all_targets, all_predictions, average='weighted')\n    sensitivity = recall_score(all_targets, all_predictions, average='weighted')\n    precision = precision_score(all_targets, all_predictions, average='weighted')\n\n    conf_matrix = confusion_matrix(all_targets, all_predictions)\n    tn = conf_matrix.sum() - conf_matrix.sum(axis=0) - conf_matrix.sum(axis=1) + np.diagonal(conf_matrix)\n    fp = conf_matrix.sum(axis=0) - np.diagonal(conf_matrix)\n    specificity_per_class = tn / (tn + fp + 1e-6)\n    samples_per_class = conf_matrix.sum(axis=1)\n    total_samples = np.sum(samples_per_class)\n    specificity = np.sum((samples_per_class / total_samples) * specificity_per_class)\n\n    return accuracy, f1, sensitivity, precision, specificity, conf_matrix,elapsed_time\n\ndef plot_acc_loss(history):\n    epochs = range(1, len(history['train_acc']) + 1)\n\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Accuracy (%)', color='tab:blue')\n    ax1.plot(epochs, history['train_acc'], label='Train Accuracy', color='tab:blue', linestyle='-')\n    ax1.plot(epochs, history['val_acc'], label='Val Accuracy', color='tab:blue', linestyle='--')\n    ax1.tick_params(axis='y', labelcolor='tab:blue')\n    ax1.set_ylim(90,100)\n    ax1.grid(True)\n    \n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Loss', color='tab:red')\n    ax2.plot(epochs, history['train_loss'], label='Train Loss', color='tab:red', linestyle='-')\n    ax2.plot(epochs, history['val_loss'], label='Val Loss', color='tab:red', linestyle='--')\n    ax2.tick_params(axis='y', labelcolor='tab:red')\n    ax2.set_ylim(0,0.03)\n    lines1, labels1 = ax1.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n\n    plt.title('Accuracy and Loss over Epochs')\n    fig.tight_layout()\n    plt.savefig(\"Resnet+LSTM Accuracy_Loss Dual Axis.png\", dpi=300, bbox_inches='tight', pad_inches=0.1)\n    plt.show()\n\n\ndef train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs):\n    history = {\n        'train_acc': [], 'val_acc': [],\n        'train_loss': [], 'val_loss': []\n    }\n    best_val_loss = float('inf') \n    best_model_path = 'Resnet+LSTM_best_model.pth'\n    for epoch in range(num_epochs):\n        # === Training ===\n        model.train()\n        running_loss = 0.0\n        correct_train = 0\n        total_train = 0\n\n        for data, target in train_loader:\n            optimizer.zero_grad()\n            data = data.to(device)\n            target = target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            _, predicted = torch.max(output, 1)\n            total_train += target.size(0)\n            correct_train += (predicted == target).sum().item()\n\n        train_loss = running_loss / len(train_loader)\n        train_acc = 100 * correct_train / total_train\n        history['train_loss'].append(train_loss)\n        history['train_acc'].append(train_acc)      \n        # === Validation ===\n        model.eval()\n        val_loss = 0.0\n        correct_val = 0\n        total_val = 0\n\n        with torch.no_grad():\n            for data, target in val_loader:\n                data = data.to(device)\n                target = target.to(device)\n                output = model(data)\n                loss = criterion(output, target)\n                val_loss += loss.item()\n                _, predicted = torch.max(output, 1)\n                total_val += target.size(0)\n                correct_val += (predicted == target).sum().item()\n\n        val_loss /= len(val_loader)\n        val_acc = 100 * correct_val / total_val\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n\n        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n        print(f\"  Train -> Acc: {train_acc:.2f}% | Loss: {train_loss:.4f}\")\n        print(f\"  Val   -> Acc: {val_acc:.2f}% | Loss: {val_loss:.4f}\")\n       # === Save Best Model (based on validation loss) ===\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), best_model_path)\n            print(\"  âœ… Best model saved (state_dict).\")\n        else:\n            print(\"  No improvement.\")\n\n    # === é‡æ–°åŠ è½½æœ€ä¼˜æ¨¡åž‹è¿›è¡Œæµ‹è¯• ===\n    model.load_state_dict(torch.load(best_model_path))\n    model.eval()\n\n    # === æµ‹è¯•é›†è¯„ä¼° ===\n    start_time = time.time()\n    test_acc, test_f1, test_sen, test_ppv, test_spe, conf_matrix, test_time = compute_test_metrics(model, test_loader)\n    test_time = time.time() - start_time\n\n    print(\"\\n=== Final Test Set Performance ===\")\n    print(f\"Test Acc: {test_acc:.2f}% | F1: {test_f1:.4f} | Sensitivity: {test_sen:.4f} | \"\n          f\"PPV: {test_ppv:.4f} | Specificity: {test_spe:.4f}\")\n    print(f\"ðŸ•’ Inference Time on Test Set: {test_time:.2f} seconds\")\n\n    # === è®¡ç®—æ¨¡åž‹å‚æ•°é‡å’Œ FLOPs ===\n    print(\"\\n=== Model Complexity ===\")\n    macs, params = get_model_complexity_info(\n        model, input_res=(1, 300), as_strings=True,\n        print_per_layer_stat=False, verbose=False\n    )\n    print(f\"ðŸ“Š Params: {params}\")\n    print(f\"ðŸ“Š FLOPs: {macs}\")\n\n    # === ä¿å­˜æ¨¡åž‹ç»“æž„å¤æ‚åº¦ä¿¡æ¯åˆ°æ–‡ä»¶ ===\n    with open(\"Resnet+LSTM_Model_Complexity.txt\", \"w\") as f:\n        f.write(f\"Params: {params}\\nFLOPs: {macs}\\nTest Time: {test_time:.2f} sec\")\n\n    return model, history\n\ntrain_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs=50)\n        ","metadata":{"ExecutionIndicator":{"show":true},"execution":{"iopub.status.busy":"2025-07-08T12:46:38.408843Z","iopub.execute_input":"2025-07-08T12:46:38.409523Z","iopub.status.idle":"2025-07-08T12:50:49.414054Z","shell.execute_reply.started":"2025-07-08T12:46:38.409489Z","shell.execute_reply":"2025-07-08T12:50:49.413241Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ptflops in /usr/local/lib/python3.11/dist-packages (0.7.4)\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from ptflops) (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->ptflops) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->ptflops) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->ptflops) (3.0.2)\nTrain: 76483, Val: 21852, Test: 10927\nEpoch [1/50]\n  Train -> Acc: 83.49% | Loss: 0.0642\n  Val   -> Acc: 85.76% | Loss: 0.0504\n  âœ… Best model saved (state_dict).\nEpoch [2/50]\n  Train -> Acc: 86.06% | Loss: 0.0479\n  Val   -> Acc: 86.84% | Loss: 0.0447\n  âœ… Best model saved (state_dict).\nEpoch [3/50]\n  Train -> Acc: 87.30% | Loss: 0.0435\n  Val   -> Acc: 87.60% | Loss: 0.0409\n  âœ… Best model saved (state_dict).\nEpoch [4/50]\n  Train -> Acc: 87.56% | Loss: 0.0406\n  Val   -> Acc: 88.07% | Loss: 0.0391\n  âœ… Best model saved (state_dict).\nEpoch [5/50]\n  Train -> Acc: 88.38% | Loss: 0.0380\n  Val   -> Acc: 90.78% | Loss: 0.0321\n  âœ… Best model saved (state_dict).\nEpoch [6/50]\n  Train -> Acc: 91.58% | Loss: 0.0300\n  Val   -> Acc: 91.94% | Loss: 0.0271\n  âœ… Best model saved (state_dict).\nEpoch [7/50]\n  Train -> Acc: 92.00% | Loss: 0.0275\n  Val   -> Acc: 92.59% | Loss: 0.0250\n  âœ… Best model saved (state_dict).\nEpoch [8/50]\n  Train -> Acc: 92.22% | Loss: 0.0262\n  Val   -> Acc: 92.45% | Loss: 0.0264\n  No improvement.\nEpoch [9/50]\n  Train -> Acc: 92.49% | Loss: 0.0250\n  Val   -> Acc: 92.23% | Loss: 0.0240\n  âœ… Best model saved (state_dict).\nEpoch [10/50]\n  Train -> Acc: 92.78% | Loss: 0.0239\n  Val   -> Acc: 92.06% | Loss: 0.0238\n  âœ… Best model saved (state_dict).\nEpoch [11/50]\n  Train -> Acc: 92.94% | Loss: 0.0231\n  Val   -> Acc: 93.34% | Loss: 0.0212\n  âœ… Best model saved (state_dict).\nEpoch [12/50]\n  Train -> Acc: 93.03% | Loss: 0.0219\n  Val   -> Acc: 93.46% | Loss: 0.0197\n  âœ… Best model saved (state_dict).\nEpoch [13/50]\n  Train -> Acc: 93.20% | Loss: 0.0210\n  Val   -> Acc: 93.85% | Loss: 0.0196\n  âœ… Best model saved (state_dict).\nEpoch [14/50]\n  Train -> Acc: 93.35% | Loss: 0.0204\n  Val   -> Acc: 93.89% | Loss: 0.0184\n  âœ… Best model saved (state_dict).\nEpoch [15/50]\n  Train -> Acc: 93.56% | Loss: 0.0199\n  Val   -> Acc: 94.05% | Loss: 0.0181\n  âœ… Best model saved (state_dict).\nEpoch [16/50]\n  Train -> Acc: 93.85% | Loss: 0.0187\n  Val   -> Acc: 94.18% | Loss: 0.0174\n  âœ… Best model saved (state_dict).\nEpoch [17/50]\n  Train -> Acc: 94.27% | Loss: 0.0179\n  Val   -> Acc: 94.39% | Loss: 0.0164\n  âœ… Best model saved (state_dict).\nEpoch [18/50]\n  Train -> Acc: 94.67% | Loss: 0.0168\n  Val   -> Acc: 95.03% | Loss: 0.0155\n  âœ… Best model saved (state_dict).\nEpoch [19/50]\n  Train -> Acc: 94.77% | Loss: 0.0164\n  Val   -> Acc: 95.26% | Loss: 0.0141\n  âœ… Best model saved (state_dict).\nEpoch [20/50]\n  Train -> Acc: 94.93% | Loss: 0.0159\n  Val   -> Acc: 95.52% | Loss: 0.0140\n  âœ… Best model saved (state_dict).\nEpoch [21/50]\n  Train -> Acc: 95.15% | Loss: 0.0151\n  Val   -> Acc: 95.24% | Loss: 0.0149\n  No improvement.\nEpoch [22/50]\n  Train -> Acc: 95.36% | Loss: 0.0147\n  Val   -> Acc: 94.86% | Loss: 0.0147\n  No improvement.\nEpoch [23/50]\n  Train -> Acc: 95.43% | Loss: 0.0142\n  Val   -> Acc: 95.80% | Loss: 0.0128\n  âœ… Best model saved (state_dict).\nEpoch [24/50]\n  Train -> Acc: 95.51% | Loss: 0.0139\n  Val   -> Acc: 96.13% | Loss: 0.0122\n  âœ… Best model saved (state_dict).\nEpoch [25/50]\n  Train -> Acc: 95.72% | Loss: 0.0135\n  Val   -> Acc: 95.79% | Loss: 0.0123\n  No improvement.\nEpoch [26/50]\n  Train -> Acc: 95.75% | Loss: 0.0132\n  Val   -> Acc: 96.16% | Loss: 0.0123\n  No improvement.\nEpoch [27/50]\n  Train -> Acc: 95.85% | Loss: 0.0129\n  Val   -> Acc: 96.27% | Loss: 0.0117\n  âœ… Best model saved (state_dict).\nEpoch [28/50]\n  Train -> Acc: 96.04% | Loss: 0.0125\n  Val   -> Acc: 96.41% | Loss: 0.0110\n  âœ… Best model saved (state_dict).\nEpoch [29/50]\n  Train -> Acc: 96.04% | Loss: 0.0122\n  Val   -> Acc: 96.27% | Loss: 0.0111\n  No improvement.\nEpoch [30/50]\n  Train -> Acc: 96.14% | Loss: 0.0120\n  Val   -> Acc: 96.39% | Loss: 0.0110\n  No improvement.\nEpoch [31/50]\n  Train -> Acc: 96.26% | Loss: 0.0118\n  Val   -> Acc: 96.49% | Loss: 0.0108\n  âœ… Best model saved (state_dict).\nEpoch [32/50]\n  Train -> Acc: 96.24% | Loss: 0.0116\n  Val   -> Acc: 96.50% | Loss: 0.0104\n  âœ… Best model saved (state_dict).\nEpoch [33/50]\n  Train -> Acc: 96.26% | Loss: 0.0114\n  Val   -> Acc: 96.66% | Loss: 0.0107\n  No improvement.\nEpoch [34/50]\n  Train -> Acc: 96.39% | Loss: 0.0111\n  Val   -> Acc: 96.71% | Loss: 0.0099\n  âœ… Best model saved (state_dict).\nEpoch [35/50]\n  Train -> Acc: 96.43% | Loss: 0.0110\n  Val   -> Acc: 96.62% | Loss: 0.0104\n  No improvement.\nEpoch [36/50]\n  Train -> Acc: 96.58% | Loss: 0.0108\n  Val   -> Acc: 96.98% | Loss: 0.0096\n  âœ… Best model saved (state_dict).\nEpoch [37/50]\n  Train -> Acc: 96.55% | Loss: 0.0106\n  Val   -> Acc: 96.64% | Loss: 0.0098\n  No improvement.\nEpoch [38/50]\n  Train -> Acc: 96.63% | Loss: 0.0103\n  Val   -> Acc: 97.00% | Loss: 0.0094\n  âœ… Best model saved (state_dict).\nEpoch [39/50]\n  Train -> Acc: 96.65% | Loss: 0.0103\n  Val   -> Acc: 97.13% | Loss: 0.0087\n  âœ… Best model saved (state_dict).\nEpoch [40/50]\n  Train -> Acc: 96.76% | Loss: 0.0100\n  Val   -> Acc: 97.10% | Loss: 0.0090\n  No improvement.\nEpoch [41/50]\n  Train -> Acc: 96.80% | Loss: 0.0099\n  Val   -> Acc: 96.92% | Loss: 0.0098\n  No improvement.\nEpoch [42/50]\n  Train -> Acc: 96.85% | Loss: 0.0098\n  Val   -> Acc: 96.83% | Loss: 0.0094\n  No improvement.\nEpoch [43/50]\n  Train -> Acc: 96.85% | Loss: 0.0096\n  Val   -> Acc: 97.28% | Loss: 0.0085\n  âœ… Best model saved (state_dict).\nEpoch [44/50]\n  Train -> Acc: 96.96% | Loss: 0.0094\n  Val   -> Acc: 96.90% | Loss: 0.0089\n  No improvement.\nEpoch [45/50]\n  Train -> Acc: 96.92% | Loss: 0.0095\n  Val   -> Acc: 97.23% | Loss: 0.0085\n  No improvement.\nEpoch [46/50]\n  Train -> Acc: 96.95% | Loss: 0.0092\n  Val   -> Acc: 97.23% | Loss: 0.0083\n  âœ… Best model saved (state_dict).\nEpoch [47/50]\n  Train -> Acc: 97.01% | Loss: 0.0091\n  Val   -> Acc: 97.30% | Loss: 0.0081\n  âœ… Best model saved (state_dict).\nEpoch [48/50]\n  Train -> Acc: 97.02% | Loss: 0.0091\n  Val   -> Acc: 97.29% | Loss: 0.0083\n  No improvement.\nEpoch [49/50]\n  Train -> Acc: 97.12% | Loss: 0.0088\n  Val   -> Acc: 97.43% | Loss: 0.0079\n  âœ… Best model saved (state_dict).\nEpoch [50/50]\n  Train -> Acc: 97.03% | Loss: 0.0088\n  Val   -> Acc: 97.43% | Loss: 0.0080\n  No improvement.\n\n=== Final Test Set Performance ===\nTest Acc: 97.36% | F1: 0.9722 | Sensitivity: 0.9736 | PPV: 0.9722 | Specificity: 0.9160\nðŸ•’ Inference Time on Test Set: 0.32 seconds\n\n=== Model Complexity ===\nðŸ“Š Params: 50.37 k\nðŸ“Š FLOPs: 7.69 MMac\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(ResNetLSTM(\n   (conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n   (dropout): Dropout(p=0.2, inplace=False)\n   (resnet_block1): Sequential(\n     (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n     (1): ReLU()\n     (2): Dropout(p=0.2, inplace=False)\n     (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n     (4): ReLU()\n     (5): Dropout(p=0.2, inplace=False)\n   )\n   (resnet_block2): Sequential(\n     (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n     (1): ReLU()\n     (2): Dropout(p=0.2, inplace=False)\n     (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n     (4): ReLU()\n     (5): Dropout(p=0.2, inplace=False)\n   )\n   (resnet_block3): Sequential(\n     (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n     (1): ReLU()\n     (2): Dropout(p=0.2, inplace=False)\n     (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n     (4): ReLU()\n     (5): Dropout(p=0.2, inplace=False)\n   )\n   (resnet_block4): Sequential(\n     (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n     (1): ReLU()\n     (2): Dropout(p=0.2, inplace=False)\n     (3): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n     (4): ReLU()\n     (5): Dropout(p=0.2, inplace=False)\n   )\n   (global_avg_pool): AdaptiveAvgPool1d(output_size=1)\n   (lstm): LSTM(32, 64, batch_first=True)\n   (fc): Linear(in_features=64, out_features=5, bias=True)\n ),\n {'train_acc': [83.49437129819698,\n   86.05572480158989,\n   87.29652341043108,\n   87.55801942915419,\n   88.3804244080384,\n   91.5798281971157,\n   92.00345174744714,\n   92.21918596289372,\n   92.48852686217852,\n   92.78401736333565,\n   92.94352993475674,\n   93.02851614084176,\n   93.20110351319902,\n   93.35015624387118,\n   93.56327549913053,\n   93.84961363963234,\n   94.27323718996378,\n   94.67201861851653,\n   94.77269458572493,\n   94.92959219695881,\n   95.15055633277983,\n   95.36367558803917,\n   95.43166455290718,\n   95.50880587843051,\n   95.72061765359622,\n   95.75330465593662,\n   95.853980623145,\n   96.04487271681289,\n   96.04356523671927,\n   96.1390112835532,\n   96.25799197207222,\n   96.24491717113607,\n   96.26452937254031,\n   96.38873998143379,\n   96.43057934442947,\n   96.57832459500804,\n   96.54956003294849,\n   96.62670135847182,\n   96.65415844043774,\n   96.75875684792699,\n   96.79798125073546,\n   96.8515879345737,\n   96.84635801419924,\n   96.96141626243741,\n   96.92349933972255,\n   96.95487886196932,\n   97.00717806571394,\n   97.02156034674373,\n   97.12354379404573,\n   97.03071270739903],\n  'val_acc': [85.76331685886875,\n   86.83873329672342,\n   87.60296540362438,\n   88.06974190005492,\n   90.77887607541643,\n   91.9366648361706,\n   92.59106717920557,\n   92.45377997437305,\n   92.23412044664104,\n   92.06022332051987,\n   93.3369943254622,\n   93.45597656965037,\n   93.84953322350357,\n   93.89071938495333,\n   94.05088779059125,\n   94.18359875526268,\n   94.38952956251144,\n   95.02562694490207,\n   95.26359143327842,\n   95.5198608822991,\n   95.240710232473,\n   94.85630605894197,\n   95.7990115321252,\n   96.13307706388431,\n   95.78985905180303,\n   96.15595826468973,\n   96.27036426871682,\n   96.41222771371042,\n   96.27036426871682,\n   96.38934651290499,\n   96.48544755628775,\n   96.49917627677101,\n   96.65934468240893,\n   96.70968332418086,\n   96.62273476112027,\n   96.98425773384587,\n   96.63646348160351,\n   97.00256269449021,\n   97.13069741900055,\n   97.09866373787297,\n   96.9201903715907,\n   96.82866556836903,\n   97.27713710415523,\n   96.89730917078528,\n   97.2267984623833,\n   97.23137470254439,\n   97.30001830496064,\n   97.28628958447739,\n   97.42815302947099,\n   97.43272926963208],\n  'train_loss': [0.06417048372714515,\n   0.04789338655221622,\n   0.04349820221148207,\n   0.0405540937669949,\n   0.03802675348286046,\n   0.030011158101696734,\n   0.027467510560184816,\n   0.026210880629643548,\n   0.0250111735172034,\n   0.023937108671840866,\n   0.023094203985018973,\n   0.0219040747613562,\n   0.020979855862137766,\n   0.02037370121293393,\n   0.019884161596625646,\n   0.01873841572453246,\n   0.01792633200818081,\n   0.01681039376140737,\n   0.01643747775535096,\n   0.015935203474775006,\n   0.015091032501509456,\n   0.014737453175409632,\n   0.014239702741288421,\n   0.013919136905497606,\n   0.013521486505504735,\n   0.01316341452330139,\n   0.012908574397345525,\n   0.012531437684102824,\n   0.01222462830231852,\n   0.011964050931061765,\n   0.011794284604377672,\n   0.01164010275049403,\n   0.011416005370920418,\n   0.011056849869267148,\n   0.010967083428332264,\n   0.010780358678866862,\n   0.010586646416054783,\n   0.010308342050008763,\n   0.010312928447895449,\n   0.009990776212337772,\n   0.00987224946348277,\n   0.009776074096660151,\n   0.009609173199186806,\n   0.009415011114384792,\n   0.009475590587191806,\n   0.009224624356666088,\n   0.009110404081357625,\n   0.009085264581958,\n   0.008849832176559458,\n   0.00879762201284971],\n  'val_loss': [0.050351885544974904,\n   0.04467344013919607,\n   0.04087651944692023,\n   0.039057953092569146,\n   0.032142756659297916,\n   0.02711436706894671,\n   0.025003091846075323,\n   0.026352869983958572,\n   0.023966051186080913,\n   0.02375895900608242,\n   0.021204439716206655,\n   0.01972672238684537,\n   0.019550295165407728,\n   0.018411751389939186,\n   0.018073929413848104,\n   0.01740817586232347,\n   0.016362471639434672,\n   0.01552441784545605,\n   0.014106428939631285,\n   0.01403087633798084,\n   0.014938538708882025,\n   0.0146530797617913,\n   0.012794592016109685,\n   0.01219287801163587,\n   0.012271829385157914,\n   0.012268839700811962,\n   0.011735367945433534,\n   0.010962449146260381,\n   0.011110963753277534,\n   0.010972137662326123,\n   0.010782794617061988,\n   0.010442361834034193,\n   0.010652081064651148,\n   0.009943270191212583,\n   0.010365069525218323,\n   0.009616257522080899,\n   0.009842753267373171,\n   0.009386148881739038,\n   0.008740335606223745,\n   0.008980704592649786,\n   0.009750966566794535,\n   0.009382050051435567,\n   0.008492186169918866,\n   0.008945938583724854,\n   0.008511100762267734,\n   0.008328239949507236,\n   0.008115997086605873,\n   0.00830536594407426,\n   0.007938465326125816,\n   0.007979235442338936]})"},"metadata":{}}],"execution_count":5},{"id":"e6bbef2b-a6a6-4ac5-a776-aa88c6876037","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}